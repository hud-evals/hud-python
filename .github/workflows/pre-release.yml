name: Pre-Release Checks

permissions:
  contents: read

on:
  workflow_dispatch:
  push:
    tags:
      - 'v*-rc*'  # Triggers on release candidate tags like v1.0.0-rc1
      - 'v*-beta*' # Triggers on beta tags like v1.0.0-beta1
      - 'v*-alpha*' # Triggers on alpha tags like v1.0.0-alpha1
  pull_request:
    types: [ready_for_review]
    branches: [main]

jobs:
  evaluation-check:
    name: Run Evaluation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    environment: pre-release  # Uses the pre-release environment
    # Only run if it's a tag push, manual trigger, or PR marked ready for review
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && github.event.action == 'ready_for_review')
    
    steps:
      - name: Check out code
        uses: actions/checkout@v4
      
      - name: Install uv
        uses: astral-sh/setup-uv@v5
      
      - name: Install Python
        run: uv python install 3.12
      
      - name: Setup virtual display
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          sleep 3
      
      - name: Run pre-release evaluation check
        env:
          HUD_API_KEY: ${{ secrets.HUD_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          DISPLAY: :99
          XAUTHORITY: /dev/null
        run: |
          # Run the pre-release check script
          uv run --with=".[agent]" python scripts/pre_release_check.py | tee evaluation_output.log
          
          # The script will exit with code 0 on success, 1 on failure
          exit ${PIPESTATUS[0]}
      
      - name: Upload evaluation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-logs
          path: evaluation_output.log
          retention-days: 7
      
      - name: Create summary
        if: always()
        run: |
          echo "## Pre-Release Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f evaluation_output.log ]; then
            # Extract the evaluation summary section
            echo "### Summary" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            sed -n '/EVALUATION SUMMARY/,/^=/p' evaluation_output.log >> $GITHUB_STEP_SUMMARY || echo "No summary found" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Check final status
            if grep -q "All pre-release checks passed!" evaluation_output.log; then
              echo "### ✅ Status: PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "### ❌ Status: FAILED" >> $GITHUB_STEP_SUMMARY
              echo "Check the uploaded evaluation logs for details." >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### ❌ Evaluation Failed" >> $GITHUB_STEP_SUMMARY
            echo "No evaluation output was generated." >> $GITHUB_STEP_SUMMARY
          fi

  notify-success:
    name: Notify Success
    needs: evaluation-check
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: Success notification
        run: |
          echo "✅ All pre-release checks passed successfully!"
          echo "The release candidate is ready for further testing."
