---
title: "Introduction"
description: "Build, evaluate, and train AI agents."
icon: "book"
---

HUD gives you three things: access to every model through one API, a way to turn your code into agent-callable tools, and infrastructure to run evaluations and training at scale.

## Install

```bash
# Install CLI
uv tool install hud-python --python 3.12

# Set your API key
hud set HUD_API_KEY=your-key-here
```

Get your API key at [hud.ai/project/api-keys](https://hud.ai/project/api-keys).

## 1. Models: Any Model, One API

Stop juggling API keys. Point any OpenAI-compatible client at `inference.hud.ai` and use Claude, GPT, Gemini, or Grok. Browse all available models at [hud.ai/models](https://hud.ai/models).

```python
from openai import AsyncOpenAI
import os

client = AsyncOpenAI(
    base_url="https://inference.hud.ai",
    api_key=os.environ["HUD_API_KEY"]
)

response = await client.chat.completions.create(
    model="claude-sonnet-4-5",  # or gpt-4o, gemini-2.5-pro, grok-4-1-fast...
    messages=[{"role": "user", "content": "Hello!"}]
)
```

Every call is traced. View them at [hud.ai/home](https://hud.ai/home).

â†’ [More on Models](/quick-links/models)

## 2. Environments: Your Code, Agent-Ready

A production API is one live instance with shared stateâ€”you can't run 1,000 parallel tests without them stepping on each other. Environments spin up fresh for every evaluation: isolated, deterministic, reproducible. Each generates training data.

Turn your code into tools agents can call. Define scenarios that evaluate what agents do:

```python
from hud import Environment

env = Environment("my-env")

@env.tool()
def search(query: str) -> str:
    """Search the knowledge base."""
    return db.search(query)

@env.scenario("find-answer")
async def find_answer(question: str):
    answer = yield f"Find the answer to: {question}"
    yield 1.0 if "correct" in answer.lower() else 0.0
```

Iterate locally with `hud dev`, then deploy to the platform:

```bash
hud init          # Scaffold environment
hud dev env:env -w env.py -w tools/  # Run MCP server with hot-reload on watched paths
hud deploy        # Deploy to platform â†’ run evals at scale
```

Once deployed, your environment is liveâ€”agents can run against it in parallel, all traced, all generating training data.

â†’ [More on Environments](/quick-links/environments) Â· [Hosted Running](/quick-links/deploy)

## 3. Tasks & Training: Test and Train

Create tasks from your scenarios on [hud.ai](https://hud.ai). Run evaluations across models. Train on successful completions. The same model string works before and after trainingâ€”just better at your tasks.

â†’ [More on Tasks & Training](/quick-links/evals)

## Next Steps

<CardGroup cols={2}>
<Card title="Models" icon="robot" href="/quick-links/models">
  One endpoint for every model. Native tools.
</Card>

<Card title="Environments" icon="cube" href="/quick-links/environments">
  Tools, scenarios, and iteration.
</Card>

<Card title="Hosted Running" icon="rocket" href="/quick-links/deploy">
  Push to platform. Run at scale.
</Card>

<Card title="Tasks & Training" icon="flask-vial" href="/quick-links/evals">
  Evaluate and train models.
</Card>
</CardGroup>

## Community

<CardGroup cols={2}>
<Card title="GitHub" icon="github" href="https://github.com/hud-evals/hud-python">
  Star the repo and contribute
</Card>

<Card title="Discord" icon="discord" href="https://discord.gg/wkjtmHYYjm">
  Join the community
</Card>
</CardGroup>

## Enterprise

Building agents at scale? We work with teams on custom environments, benchmarks, and training pipelines.

[ðŸ“… Book a call](https://cal.com/jay-hud) Â· [ðŸ“§ founders@hud.ai](mailto:founders@hud.ai)
