---
title: "Quickstart"
description: "Run your first agent evaluation in 3 minutes"
icon: "bolt"
---

Get up and running with HUD in minutes. Follow these four steps to install the CLI and run your first evaluation.

<Steps>
<Step title="Install uv">
  HUD uses `uv` for fast, reliable Python package management.

  <Tabs>
  <Tab title="macOS / Linux">
  ```bash
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```
  </Tab>
  <Tab title="Windows">
  ```powershell
  powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
  ```
  </Tab>
  </Tabs>
</Step>

<Step title="Install HUD Tool">
  Install the HUD CLI globally using `uv`:

  ```bash
  uv tool install hud-python@latest
  ```
</Step>

<Step title="Get and Set API Keys">
  1. Get your **HUD API Key** from [hud.ai](https://hud.ai)
  2. Get your **Anthropic API Key** from [console.anthropic.com](https://console.anthropic.com/settings/keys)
  3. Set them using the CLI:

  ```bash
  hud set HUD_API_KEY=sk-hud-... ANTHROPIC_API_KEY=sk-ant-...
  ```
</Step>

<Step title="Run Your First Evaluation">
  Run the standard SheetBench-50 benchmark using Claude:

  ```bash
  hud eval hud-evals/SheetBench-50 claude
  ```

  <Note>
  This will stream results to your terminal and the [HUD Dashboard](https://hud.ai).
  </Note>
</Step>
</Steps>

## Environments/CLI Quick Reference

```bash
# Create sample environment
hud init

# Change directory to the environment and build the docker container
cd sample && hud build

# Start hot-reload development server for an environment
hud dev . --build --interactive

# Run a task in your environment
hud eval claude
```

## What just happened?

1. **Task Definition**: We created a `Task` with:
   - A prompt telling the agent what to do
   - An `mcp_config` pointing to a remote MCP environment
   - Setup and evaluation tools to initialize and grade the trajectory

2. **Auto Client**: The agent automatically created an MCP client from `task.mcp_config`

3. **Telemetry**: The `trace` context captured all interactions for debugging

4. **Evaluation**: The `evaluate` tool returns a reward at the end of the agent loop

## Next Steps

<CardGroup cols={2}>
<Card title="Understand MCP" icon="book" href="/core-concepts/mcp-protocol">
  Learn how agents connect to environments
</Card>

<Card title="Run Benchmarks" icon="chart-line" href="/evaluate-agents/benchmarks">
  Test on SheetBench and OSWorld
</Card>

<Card title="Build Environments" icon="cube" href="/build-environments">
  Create your own MCP environment
</Card>

<Card title="GitHub" icon="github" href="https://github.com/hud-evals/hud-python">
  Star us and contribute!
</Card>
</CardGroup>


<Note>
See the [CLI Reference](/reference/cli/overview) for detailed command documentation
</Note>
