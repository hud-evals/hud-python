---
title: "RL Quickstart"
icon: "graduation-cap"
---

## Prerequisites

- HUD API key: Remote training requires authentication. Set `HUD_API_KEY` before running:

```bash
export HUD_API_KEY="sk-hud-..."  # get one at https://hud.so
# Or persist it locally:
hud set HUD_API_KEY=sk-hud-...
```

- Docker daemon: For local runs (using `--local`) or when training against a local Docker image, ensure Docker Desktop is installed and the Docker daemon is running.

## Quickstart

Install and download a taskset:

```bash
uv tool install hud-python
hud get hud-evals/2048-basic
```

### 1) Simple: Train

```bash
hud rl 2048-basic.json
```

This launches training remotely and automatically provisions a vLLM server and a trainer for you. You can monitor progress on https://hud.so. The server persists between runs, so you can rerun training or evaluate against the same endpoint.

Optional baseline first (Claude or Operator):

```bash
hud eval 2048-basic.json
```

### 2) Build your own environment (hud init)

Create a new MCP environment, develop with hot-reload, and train on a production image:

```bash
hud init my-env && cd my-env
hud dev --interactive
# When ready to run:
hud rl
```

Change the tasks.json to include other tasks you want to train on.

See [hud init](/reference/cli/init) for options and details.


## Getting the best performance

Often training a good model requires many iterations over the parameters of the trainer. Take the config generated by `hud rl` and modify it to various values to do a hyperparameter sweep.

For easy launching, specify the tasks and config upfront, and add `--yes` to automatically launch vllm and training.

```bash
hud rl taskset.json --config rl-config.json --yes
```

Additionally, sometimes it may be helpful to run an initial analysis on the dataset to determine which tasks would be the most informative to train on. If you already have a deployed model you can evaluate it directly, otherwise launch training briefly, then:

```bash
hud eval taskset.json --full --group-size 6 --max-steps 5
```

This will prompt you for the model choice, produce a table of accuracies per task. Prefer tasks which are 10%-60% accurate for training.

Some general findings from our internal training runs:
- As many different tasks per gradient update as possible (runs with 4+ GPUs and batch size of 50+ are much more stable than single GPU runs)
- Batch size should be somewhere around 2/X where X is the accuracy of that given task on an untrained model.

### Pricing

Below is the pricing by GPU type. Actual prices vary â€” see https://hud.so/project/billing for current rates.

vLLM GPU Pricing (2 Hosted GPUs)

| GPU type | Memory | Est. price/hr |
| --- | --- | --- |
| A100 80GB | 80 GB | $4.95 |
| H100 80GB | 80 GB | $7.95 |

Training GPU Pricing

| GPU type | Memory | Est. price/hr |
| --- | --- | --- |
| A100 80GB | 80 GB | $3.95 |
| H100 80GB | 80 GB | $5.40 |

---

### Learn more

<CardGroup cols={2}>
<Card title="Environment Quickstart" icon="cube" href="/build-environments">
  Complete guide to building environments from scratch
</Card>

<Card title="RL CLI Reference" icon="terminal" href="/reference/cli/rl">
  Full `hud rl` command options and usage
</Card>
</CardGroup>
