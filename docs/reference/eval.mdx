---
title: "Evaluation API"
description: "SDK reference for running evaluations with hud.eval()"
icon: "flask-vial"
---

The HUD SDK provides a unified evaluation API through `hud.eval()` for tracking agent performance, running parallel evaluations, and integrating with the HUD platform.

## Overview

There are three ways to run evaluations:

1. **`hud.eval()`** - Standalone context manager for any evaluation
2. **`env.eval()`** - Method on `Environment` for evaluating within an existing environment
3. **`run_tasks()`** - High-level batch execution with automatic agent creation

## hud.eval()

The primary evaluation context manager. Creates an `EvalContext` which is a full `Environment` with evaluation tracking.

```python
import hud

async with hud.eval("my-org/browser-task:1") as ctx:
    # ctx is an EvalContext (extends Environment)
    tools = await ctx.list_tools()
    result = await ctx.call_tool("navigate", url="https://example.com")
    ctx.reward = 1.0  # Set the evaluation reward
```

### Parameters

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `source` | `str \| list[str] \| Task \| list[Task] \| None` | Task source (slugs or Task objects) | `None` |
| `variants` | `dict[str, Any] \| None` | A/B test configuration | `None` |
| `group` | `int` | Runs per variant for statistical significance | `1` |
| `group_ids` | `list[str] \| None` | Custom group IDs for parallel runs | `None` |
| `job_id` | `str \| None` | Job ID to link traces to | `None` |
| `api_key` | `str \| None` | API key for backend calls | `None` |
| `max_concurrent` | `int \| None` | Maximum concurrent evaluations | `None` |

### Task Sources

The `source` parameter accepts multiple formats:

```python
# 1. Blank evaluation (manual reward)
async with hud.eval() as ctx:
    ctx.reward = compute_reward()

# 2. Single task slug
async with hud.eval("my-org/browser-task") as ctx:
    await agent.run(ctx)

# 3. Task at specific index
async with hud.eval("my-org/evalset:0") as ctx:
    await agent.run(ctx)

# 4. All tasks in an evalset (wildcard)
async with hud.eval("my-org/evalset:*") as ctx:
    await agent.run(ctx)

# 5. Multiple slugs
async with hud.eval(["task:0", "task:1", "task:2"]) as ctx:
    await agent.run(ctx)

# 6. Task objects directly (backwards compatible)
from hud.types import Task
tasks = [Task(prompt="Navigate to docs", mcp_config={...})]
async with hud.eval(tasks) as ctx:
    await agent.run(ctx)
```

### Variants and Groups

Run A/B tests with multiple configurations:

```python
# Test different models
async with hud.eval(
    "my-org/evalset:*",
    variants={"model": ["gpt-4o", "claude-sonnet"]},
    group=3,  # 3 runs per variant for statistical significance
) as ctx:
    model = ctx.variants["model"]  # Current variant assignment
    agent = create_agent(model=model)
    result = await agent.run(ctx)
    ctx.reward = result.reward

# Access all results after completion
for result in ctx.results:
    print(f"{result.variants}: reward={result.reward}")
```

**How it works:**
- `variants` dict with list values creates the cartesian product
- `group` multiplies each variant combination
- Total runs = `len(tasks) × len(variant_combos) × group`
- For parallel runs (total > 1), a job is automatically created

### Concurrency Control

Limit concurrent evaluations to manage resources:

```python
async with hud.eval(
    "my-org/large-evalset:*",
    max_concurrent=10,  # Max 10 parallel evaluations
) as ctx:
    await agent.run(ctx)
```

## env.eval()

Create evaluation contexts from an existing `Environment`:

```python
from hud import Environment

async with Environment() as env:
    # Connect to MCP servers
    await env.connect_hub("test-browser-26")
    
    # Run evaluation within this environment
    async with env.eval("my-evaluation", group=3) as ctx:
        # ctx inherits env's connections
        tools = await ctx.list_tools()
        await agent.run(ctx)
        ctx.reward = result.reward
```

### Parameters

Same as `hud.eval()`, plus:

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `name` | `str` | Evaluation name (required) | Required |
| `trace_id` | `str \| None` | Custom trace ID | `None` |

### Connection Inheritance

When you call `env.eval()`, the `EvalContext` copies the parent environment's connections:

```python
async with Environment() as env:
    await env.connect_hub("my-hub")
    
    # Parallel evaluations each get their own connection copies
    async with env.eval("test", group=3) as ctx:
        # Each parallel run has independent connections
        await ctx.call_tool("my_tool")
```

## EvalContext

`EvalContext` extends `Environment` with evaluation-specific functionality.

### Properties

| Property | Type | Description |
|----------|------|-------------|
| `trace_id` | `str` | Unique trace identifier |
| `eval_name` | `str` | Evaluation name |
| `job_id` | `str \| None` | Parent job ID |
| `group_id` | `str \| None` | Group ID for parallel runs |
| `index` | `int` | Index in parallel execution |
| `variants` | `dict[str, Any]` | Current variant assignment |
| `reward` | `float \| None` | Evaluation reward (settable) |
| `error` | `BaseException \| None` | Error if evaluation failed |
| `results` | `list[EvalContext] \| None` | Results from parallel runs |
| `task` | `Task \| None` | Task definition (if loaded from slug) |
| `prompt` | `str \| None` | Task prompt |
| `headers` | `dict[str, str]` | Trace headers for HTTP requests |

### Methods

All `Environment` methods are available, plus:

```python
# Set reward
ctx.reward = 1.0

# Access task configuration
if ctx.task:
    print(ctx.task.prompt)
    print(ctx.task.agent_config)  # Agent configuration hints

# Get trace headers for external HTTP calls
headers = ctx.headers  # {"Trace-Id": "...", "Trace-Parent": "..."}
```

### Creating from Task

```python
from hud.eval.context import EvalContext
from hud.types import Task

task = Task(
    prompt="Navigate to the docs page",
    mcp_config={"hud": {"url": "...", "headers": {...}}},
    setup_tool={"name": "setup", "arguments": {...}},
    evaluate_tool={"name": "evaluate", "arguments": {...}},
)

ctx = EvalContext.from_task(task)
async with ctx:
    # MCP connections configured from task.mcp_config
    # setup_tool and evaluate_tool configured
    tools = await ctx.list_tools()
```

## run_tasks()

High-level batch execution that creates agents automatically:

```python
from hud.datasets import run_tasks
from hud.types import AgentType
from hud.utils.tasks import load_tasks

# Load tasks from HuggingFace or file
tasks = load_tasks("hud-evals/SheetBench-50")

# Run with automatic agent creation
results = await run_tasks(
    tasks=tasks,
    agent_type=AgentType.CLAUDE,
    agent_params={"checkpoint_name": "claude-sonnet-4-5"},
    max_concurrent=30,
    max_steps=10,
    group_size=3,  # 3 runs per task
)
```

### Parameters

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `tasks` | `list[Task]` | List of Task objects | Required |
| `agent_type` | `AgentType` | Agent type enum | Required |
| `agent_params` | `dict[str, Any] \| None` | Agent configuration | `None` |
| `name` | `str` | Job name | `"Evaluation"` |
| `max_concurrent` | `int` | Maximum concurrent tasks | `30` |
| `metadata` | `dict[str, Any] \| None` | Job metadata | `None` |
| `max_steps` | `int` | Maximum steps per task | `10` |
| `group_size` | `int` | Runs per task | `1` |
| `remote` | `bool` | Submit to HUD platform | `False` |

### Returns

- If `group_size == 1`: `list[Trace]` - Results in task order
- If `group_size > 1`: `list[dict]` - Statistics per task group

### Remote Execution

Submit tasks to the HUD platform for remote execution:

```python
await run_tasks(
    tasks=tasks,
    agent_type=AgentType.CLAUDE,
    remote=True,  # Submit to platform
)
# Returns immediately, monitor at https://hud.ai/jobs/{job_id}
```

## Task Configuration

Tasks define the evaluation environment and success criteria:

```python
from hud.types import Task

task = Task(
    id="nav-001",
    prompt="Navigate to the documentation page",
    mcp_config={
        "hud": {
            "url": "https://mcp.hud.ai/v3/mcp",
            "headers": {
                "Authorization": "Bearer ${HUD_API_KEY}",
                "Mcp-Image": "hudpython/hud-remote-browser:latest"
            }
        }
    },
    setup_tool={
        "name": "setup",
        "arguments": {"name": "navigate", "arguments": {"url": "https://example.com"}}
    },
    evaluate_tool={
        "name": "evaluate", 
        "arguments": {"name": "url_match", "arguments": {"pattern": ".*/docs.*"}}
    },
    agent_config={
        "allowed_tools": ["playwright", "computer"],
        "system_prompt": "You are a web navigation agent."
    },
    metadata={"difficulty": "easy", "category": "navigation"}
)
```

### Task Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | `str \| None` | Unique task identifier |
| `prompt` | `str` | Task instruction |
| `mcp_config` | `dict[str, Any]` | MCP server configuration |
| `setup_tool` | `MCPToolCall \| list[MCPToolCall] \| None` | Setup tool calls |
| `evaluate_tool` | `MCPToolCall \| list[MCPToolCall] \| None` | Evaluation tool calls |
| `agent_config` | `BaseAgentConfig \| None` | Agent configuration hints |
| `metadata` | `dict[str, Any]` | Custom metadata |

### Environment Variable Substitution

MCP config supports `${VAR_NAME}` substitution:

```python
mcp_config = {
    "hud": {
        "url": "${HUD_MCP_URL:https://mcp.hud.ai/v3/mcp}",  # With default
        "headers": {
            "Authorization": "Bearer ${HUD_API_KEY}"  # From environment
        }
    }
}
```

## HTTP Instrumentation

When running inside an eval context, HTTP requests to HUD services automatically include trace headers:

```python
import httpx

async with hud.eval("test") as ctx:
    # Trace headers are automatically injected
    async with httpx.AsyncClient() as client:
        # Requests to inference.hud.ai, mcp.hud.ai include Trace-Id
        response = await client.post(
            "https://inference.hud.ai/v1/messages",
            json={...}
        )
```

This enables automatic telemetry linking without manual header management.

## Best Practices

### 1. Use Variants for A/B Testing

```python
async with hud.eval(
    "evalset:*",
    variants={
        "model": ["gpt-4o", "claude"],
        "temperature": [0.0, 0.7],
    },
    group=3,
) as ctx:
    # Runs: 2 models × 2 temps × 3 groups = 12 evaluations
    ...
```

### 2. Set Rewards Consistently

```python
async with hud.eval("task") as ctx:
    try:
        result = await agent.run(ctx)
        ctx.reward = result.reward
    except Exception as e:
        ctx.reward = 0.0  # Explicit failure reward
        raise
```

### 3. Use Concurrency Limits for Resource-Heavy Tasks

```python
async with hud.eval(
    "browser-tasks:*",
    max_concurrent=5,  # Browser instances are heavy
) as ctx:
    ...
```

### 4. Access Task Agent Config

```python
async with hud.eval("my-org/task:0") as ctx:
    if ctx.task and ctx.task.agent_config:
        # Apply task's agent hints
        allowed_tools = ctx.task.agent_config.allowed_tools
        system_prompt = ctx.task.agent_config.system_prompt
```

## See Also

- [`hud eval` CLI](/reference/cli/eval) - Command-line interface
- [Benchmarks](/evaluate-agents/benchmarks) - Creating and running benchmarks
- [Tasks](/reference/tasks) - Task configuration reference
- [Environments](/reference/environments) - Building MCP environments

